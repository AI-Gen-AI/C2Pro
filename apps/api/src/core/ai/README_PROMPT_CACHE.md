# Prompt Cache - C2Pro

Sistema de cachÃ© inteligente para prompts idÃ©nticos usando hash SHA-256.

## ğŸ¯ Â¿QuÃ© es el Prompt Cache?

El **Prompt Cache** evita llamadas redundantes a Claude API cacheando respuestas de prompts idÃ©nticos.

### Beneficios

- âœ… **Ahorro de costos**: $0 por respuestas cacheadas
- âœ… **Velocidad**: ~100x mÃ¡s rÃ¡pido (ms vs segundos)
- âœ… **ReducciÃ³n de latencia**: Respuestas instantÃ¡neas
- âœ… **Menor carga API**: Menos requests a Anthropic
- âœ… **Determinismo**: Mismos inputs â†’ mismos outputs

### CaracterÃ­sticas

- ğŸ” **Hash SHA-256** del input completo
- â±ï¸ **TTL 24 horas** (configurable)
- ğŸª **Redis + fallback memoria**
- ğŸ“Š **MÃ©tricas automÃ¡ticas** (hit/miss rate)
- ğŸ”„ **IntegraciÃ³n transparente** con AIService

---

## ğŸ—ï¸ Arquitectura

### Flujo de Cache

```
Request â†’ [Prompt Cache?] â”€Yesâ†’ Return cached
               â”‚
               No
               â†“
          [Document Cache?] â”€Yesâ†’ Return cached
               â”‚
               No
               â†“
          Call Claude API
               â†“
          Save to caches
               â†“
          Return response
```

### Capas de Cache

1. **Prompt Cache (Layer 1)**: Hash SHA-256 del input completo
2. **Document Cache (Layer 2)**: Por document_hash + task_type
3. **API Call**: Si ambos fallan

---

## ğŸ”‘ Hash SHA-256

El hash incluye **todos** los parÃ¡metros que afectan la respuesta:

```python
hash_input = {
    "prompt": "Analiza este contrato...",
    "system_prompt": "Eres un experto...",
    "temperature": 0.0,
    "max_tokens": 4096,
    "model": "claude-sonnet-4-20250514"
}
```

**Mismo hash â†’ Cache HIT**
**Hash diferente â†’ Cache MISS**

### Ejemplo

```python
from src.core.ai.prompt_cache import build_prompt_hash

# Mismo prompt, mismos parÃ¡metros
hash1 = build_prompt_hash(
    prompt="Clasifica este documento",
    temperature=0.0,
    model="claude-sonnet-4"
)

hash2 = build_prompt_hash(
    prompt="Clasifica este documento",  # Mismo
    temperature=0.0,  # Mismo
    model="claude-sonnet-4"  # Mismo
)

assert hash1 == hash2  # âœ… Cache HIT

# Cambiar un parÃ¡metro
hash3 = build_prompt_hash(
    prompt="Clasifica este documento",
    temperature=0.5,  # â† CAMBIO
    model="claude-sonnet-4"
)

assert hash1 != hash3  # âŒ Cache MISS
```

---

## ğŸ’» Uso

### OpciÃ³n 1: AutomÃ¡tico (Recomendado)

El cache estÃ¡ **habilitado por defecto** en todas las requests:

```python
from src.core.ai.service import AIService, AIRequest
from src.core.ai.model_router import TaskType

service = AIService(tenant_id=tenant_id)

# Primera llamada - Cache MISS
request = AIRequest(
    prompt="Analiza la coherencia de este proyecto...",
    task_type=TaskType.COHERENCE_ANALYSIS,
    # use_cache=True  â† Default
)

response = await service.generate(request)
# Costo: $0.0234 (llamÃ³ a la API)

# Segunda llamada (prompt idÃ©ntico) - Cache HIT
response2 = await service.generate(request)
# Costo: $0.0000 (Â¡gratis!)
# Tiempo: ~5ms vs 1500ms
```

### OpciÃ³n 2: Desactivar Cache Manualmente

```python
request = AIRequest(
    prompt="Analiza este proyecto...",
    task_type=TaskType.COHERENCE_ANALYSIS,
    use_cache=False,  # â† Desactivar cache
)

response = await service.generate(request)
# Siempre llama a la API, aunque el prompt sea idÃ©ntico
```

### OpciÃ³n 3: API Directa del Cache

```python
from src.core.ai.prompt_cache import get_prompt_cache_service

cache = get_prompt_cache_service()

# Intentar obtener del cache
cached = await cache.get_cached_response(
    prompt="Clasifica...",
    system_prompt="Eres experto...",
    temperature=0.0,
    model="claude-sonnet-4"
)

if cached:
    print(f"Cache HIT! Respuesta: {cached.content}")
    print(f"Edad: {cached.get_age_seconds()}s")
    print(f"Costo ahorrado: ${cached.cost_usd}")
else:
    # Cache MISS - llamar API
    response = await call_api(...)

    # Guardar en cache
    await cache.set_cached_response(
        prompt=prompt,
        response_content=response.content,
        model=model,
        input_tokens=response.input_tokens,
        output_tokens=response.output_tokens,
        cost_usd=response.cost_usd,
        execution_time_ms=execution_time,
    )
```

---

## ğŸ“Š MÃ©tricas y Observabilidad

### Logs Estructurados

Cada operaciÃ³n de cache genera logs:

**Cache HIT:**
```json
{
  "event": "prompt_cache_hit",
  "hash": "a1b2c3d4...",
  "age_seconds": 1234.5,
  "model": "claude-sonnet-4-20250514",
  "saved_cost_usd": 0.0234
}
```

**Cache MISS:**
```json
{
  "event": "prompt_cache_miss",
  "hash": "a1b2c3d4...",
  "prompt_length": 1024
}
```

**Cache WRITE:**
```json
{
  "event": "prompt_cached",
  "hash": "a1b2c3d4...",
  "model": "claude-sonnet-4",
  "cost_usd": 0.0234,
  "ttl_hours": 24
}
```

### MÃ©tricas

```python
from src.core.observability import record_cache_hit, record_cache_miss

# AutomÃ¡tico en PromptCacheService
# Expuesto vÃ­a Prometheus metrics
```

### EstadÃ­sticas

```python
cache = get_prompt_cache_service()
stats = await cache.get_cache_stats()

print(f"Enabled: {stats['enabled']}")
print(f"TTL: {stats['ttl_hours']} hours")
print(f"Type: {stats['cache_type']}")
```

---

## ğŸ”§ ConfiguraciÃ³n

### TTL del Cache

El TTL por defecto es **24 horas**. Puedes personalizarlo:

```python
from src.core.ai.prompt_cache import PROMPT_CACHE_TTL_SECONDS

# Default: 24 horas
print(PROMPT_CACHE_TTL_SECONDS)  # 86400

# Custom TTL al guardar
await cache.set_cached_response(
    ...,
    ttl_seconds=60 * 60 * 12,  # 12 horas
)
```

### Infraestructura

El Prompt Cache usa el `CacheService` existente:

1. **Con Redis**: Cache persistente, compartido entre workers
2. **Sin Redis**: Fallback a memoria (solo worker local)

```python
# Configurar Redis (opcional pero recomendado)
REDIS_URL=redis://localhost:6379/0
```

---

## ğŸ’° ROI y Ahorro

### Escenario Real: AnÃ¡lisis de Coherencia

**Sin cache:**
- Requests/dÃ­a: 1,000
- Costo promedio: $0.05 por anÃ¡lisis
- **Total/mes: $1,500 USD**

**Con cache (50% hit rate):**
- Requests cacheadas: 500 Ã— $0.00 = $0.00
- Requests nuevas: 500 Ã— $0.05 = $25.00
- **Total/mes: $750 USD**
- **Ahorro: $750/mes (50%)**

**Con cache (80% hit rate):**
- Requests cacheadas: 800 Ã— $0.00 = $0.00
- Requests nuevas: 200 Ã— $0.05 = $10.00
- **Total/mes: $300 USD**
- **Ahorro: $1,200/mes (80%)**

### Speedup

- Cache HIT: ~5ms
- API call: ~1,500ms
- **Speedup: 300x** ğŸš€

---

## ğŸ¯ Casos de Uso Ideales

### âœ… Usa Prompt Cache Para:

1. **AnÃ¡lisis repetidos del mismo proyecto**
   - Usuario revisa proyecto mÃºltiples veces
   - Mismo prompt, misma respuesta

2. **ClasificaciÃ³n de documentos similares**
   - Muchos documentos del mismo tipo
   - Prompts estandarizados

3. **Tareas con temperatura=0.0**
   - Respuestas deterministas
   - Mismo input â†’ mismo output

4. **Validaciones frecuentes**
   - Check de formato
   - ValidaciÃ³n de estructura

5. **APIs pÃºblicas con prompts comunes**
   - MÃºltiples usuarios, mismas preguntas
   - Cache compartido (con Redis)

### âŒ NO uses Prompt Cache Para:

1. **Temperature > 0.0**
   - Respuestas no deterministas
   - Cache no Ãºtil

2. **Prompts Ãºnicos**
   - Cada request diferente
   - 0% hit rate

3. **Datos sensibles one-time**
   - Prompts con PII
   - No beneficio del cache

4. **AnÃ¡lisis exploratorios**
   - Usuario experimenta
   - Prompts siempre cambian

---

## ğŸ” Seguridad y Privacidad

### Datos Cacheados

El cache almacena:
- âœ… Hash SHA-256 del input
- âœ… Respuesta de Claude
- âœ… Metadata (tokens, costo, tiempo)

**NO** almacena:
- âŒ API keys
- âŒ Tenant IDs en la key
- âŒ InformaciÃ³n de autenticaciÃ³n

### Aislamiento por Tenant

El cache es **compartido** entre tenants para maximizar hit rate.

Si necesitas aislamiento:
```python
# OpciÃ³n: Incluir tenant_id en el prompt
prompt = f"[Tenant: {tenant_id}] {user_prompt}"
# â†’ Hash diferente por tenant
```

### TTL y ExpiraciÃ³n

- Cache expira en 24h automÃ¡ticamente
- No hay datos obsoletos >24h
- Limpieza automÃ¡tica por Redis

### InvalidaciÃ³n Manual

```python
cache = get_prompt_cache_service()

# Invalidar entrada especÃ­fica
await cache.invalidate_cache(
    prompt=prompt,
    system_prompt=system_prompt,
    temperature=temperature,
    model=model,
)
```

---

## ğŸ“– Ejemplos Completos

Ver archivo: `apps/api/src/core/ai/example_prompt_cache.py`

Ejecutar:
```bash
cd apps/api
python -m src.core.ai.example_prompt_cache
```

Incluye:
- âœ… Cache hit/miss bÃ¡sico
- âœ… Estabilidad del hash SHA-256
- âœ… Control manual del cache
- âœ… EstadÃ­sticas y mÃ©tricas
- âœ… ComparaciÃ³n de costos

---

## ğŸ” Troubleshooting

### Problema: Cache nunca hace HIT

**Verificar:**
1. Â¿Cache habilitado?
   ```python
   service.prompt_cache.enabled  # True?
   ```

2. Â¿Prompts realmente idÃ©nticos?
   ```python
   # Verificar hashes
   hash1 = build_prompt_hash(...)
   hash2 = build_prompt_hash(...)
   print(hash1 == hash2)
   ```

3. Â¿TTL expirado?
   - Cache expira en 24h
   - Verificar logs: "prompt_cache_expired"

4. Â¿Redis configurado?
   - Sin Redis, cache solo funciona en mismo worker
   - Configurar `REDIS_URL` en environment

### Problema: Hit rate muy bajo (<10%)

**Causas posibles:**
- Temperature > 0.0 (respuestas no deterministas)
- Prompts siempre Ãºnicos
- TTL muy corto
- Cache se reinicia frecuentemente

**SoluciÃ³n:**
- Usar temperature=0.0 para tareas repetibles
- Estandarizar prompts cuando sea posible
- Configurar Redis para persistencia

### Problema: Cache usa mucha memoria

**SoluciÃ³n:**
- Configurar Redis con polÃ­tica de eviction:
  ```
  maxmemory 1gb
  maxmemory-policy allkeys-lru
  ```
- Reducir TTL si es necesario
- Monitor memoria con `redis-cli INFO memory`

---

## ğŸ“š Referencias

- [CÃ³digo: prompt_cache.py](./prompt_cache.py)
- [IntegraciÃ³n: service.py](./service.py)
- [Cache Core: core/cache.py](../../core/cache.py)
- [Ejemplos: example_prompt_cache.py](./example_prompt_cache.py)

---

## ğŸ“ Changelog

### v1.0.0 (2026-01-09)

**Implementado:**
- âœ… Hash SHA-256 del input completo
- âœ… TTL de 24 horas
- âœ… IntegraciÃ³n con AIService
- âœ… Redis + fallback memoria
- âœ… MÃ©tricas hit/miss automÃ¡ticas
- âœ… DocumentaciÃ³n completa
- âœ… Ejemplos de uso

**Estado:** âœ… READY FOR PRODUCTION

**Tarea:** CE-S2-006 - CachÃ© de Prompts IdÃ©nticos

---

**Autor:** C2Pro AI Team
**VersiÃ³n:** 1.0.0
**Ãšltima ActualizaciÃ³n:** 2026-01-09

