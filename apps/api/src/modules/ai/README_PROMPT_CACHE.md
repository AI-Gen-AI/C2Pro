# Prompt Cache - C2Pro

Sistema de cach√© inteligente para prompts id√©nticos usando hash SHA-256.

## üéØ ¬øQu√© es el Prompt Cache?

El **Prompt Cache** evita llamadas redundantes a Claude API cacheando respuestas de prompts id√©nticos.

### Beneficios

- ‚úÖ **Ahorro de costos**: $0 por respuestas cacheadas
- ‚úÖ **Velocidad**: ~100x m√°s r√°pido (ms vs segundos)
- ‚úÖ **Reducci√≥n de latencia**: Respuestas instant√°neas
- ‚úÖ **Menor carga API**: Menos requests a Anthropic
- ‚úÖ **Determinismo**: Mismos inputs ‚Üí mismos outputs

### Caracter√≠sticas

- üîê **Hash SHA-256** del input completo
- ‚è±Ô∏è **TTL 24 horas** (configurable)
- üè™ **Redis + fallback memoria**
- üìä **M√©tricas autom√°ticas** (hit/miss rate)
- üîÑ **Integraci√≥n transparente** con AIService

---

## üèóÔ∏è Arquitectura

### Flujo de Cache

```
Request ‚Üí [Prompt Cache?] ‚îÄYes‚Üí Return cached
               ‚îÇ
               No
               ‚Üì
          [Document Cache?] ‚îÄYes‚Üí Return cached
               ‚îÇ
               No
               ‚Üì
          Call Claude API
               ‚Üì
          Save to caches
               ‚Üì
          Return response
```

### Capas de Cache

1. **Prompt Cache (Layer 1)**: Hash SHA-256 del input completo
2. **Document Cache (Layer 2)**: Por document_hash + task_type
3. **API Call**: Si ambos fallan

---

## üîë Hash SHA-256

El hash incluye **todos** los par√°metros que afectan la respuesta:

```python
hash_input = {
    "prompt": "Analiza este contrato...",
    "system_prompt": "Eres un experto...",
    "temperature": 0.0,
    "max_tokens": 4096,
    "model": "claude-sonnet-4-20250514"
}
```

**Mismo hash ‚Üí Cache HIT**
**Hash diferente ‚Üí Cache MISS**

### Ejemplo

```python
from src.modules.ai.prompt_cache import build_prompt_hash

# Mismo prompt, mismos par√°metros
hash1 = build_prompt_hash(
    prompt="Clasifica este documento",
    temperature=0.0,
    model="claude-sonnet-4"
)

hash2 = build_prompt_hash(
    prompt="Clasifica este documento",  # Mismo
    temperature=0.0,  # Mismo
    model="claude-sonnet-4"  # Mismo
)

assert hash1 == hash2  # ‚úÖ Cache HIT

# Cambiar un par√°metro
hash3 = build_prompt_hash(
    prompt="Clasifica este documento",
    temperature=0.5,  # ‚Üê CAMBIO
    model="claude-sonnet-4"
)

assert hash1 != hash3  # ‚ùå Cache MISS
```

---

## üíª Uso

### Opci√≥n 1: Autom√°tico (Recomendado)

El cache est√° **habilitado por defecto** en todas las requests:

```python
from src.modules.ai.service import AIService, AIRequest
from src.modules.ai.model_router import TaskType

service = AIService(tenant_id=tenant_id)

# Primera llamada - Cache MISS
request = AIRequest(
    prompt="Analiza la coherencia de este proyecto...",
    task_type=TaskType.COHERENCE_ANALYSIS,
    # use_cache=True  ‚Üê Default
)

response = await service.generate(request)
# Costo: $0.0234 (llam√≥ a la API)

# Segunda llamada (prompt id√©ntico) - Cache HIT
response2 = await service.generate(request)
# Costo: $0.0000 (¬°gratis!)
# Tiempo: ~5ms vs 1500ms
```

### Opci√≥n 2: Desactivar Cache Manualmente

```python
request = AIRequest(
    prompt="Analiza este proyecto...",
    task_type=TaskType.COHERENCE_ANALYSIS,
    use_cache=False,  # ‚Üê Desactivar cache
)

response = await service.generate(request)
# Siempre llama a la API, aunque el prompt sea id√©ntico
```

### Opci√≥n 3: API Directa del Cache

```python
from src.modules.ai.prompt_cache import get_prompt_cache_service

cache = get_prompt_cache_service()

# Intentar obtener del cache
cached = await cache.get_cached_response(
    prompt="Clasifica...",
    system_prompt="Eres experto...",
    temperature=0.0,
    model="claude-sonnet-4"
)

if cached:
    print(f"Cache HIT! Respuesta: {cached.content}")
    print(f"Edad: {cached.get_age_seconds()}s")
    print(f"Costo ahorrado: ${cached.cost_usd}")
else:
    # Cache MISS - llamar API
    response = await call_api(...)

    # Guardar en cache
    await cache.set_cached_response(
        prompt=prompt,
        response_content=response.content,
        model=model,
        input_tokens=response.input_tokens,
        output_tokens=response.output_tokens,
        cost_usd=response.cost_usd,
        execution_time_ms=execution_time,
    )
```

---

## üìä M√©tricas y Observabilidad

### Logs Estructurados

Cada operaci√≥n de cache genera logs:

**Cache HIT:**
```json
{
  "event": "prompt_cache_hit",
  "hash": "a1b2c3d4...",
  "age_seconds": 1234.5,
  "model": "claude-sonnet-4-20250514",
  "saved_cost_usd": 0.0234
}
```

**Cache MISS:**
```json
{
  "event": "prompt_cache_miss",
  "hash": "a1b2c3d4...",
  "prompt_length": 1024
}
```

**Cache WRITE:**
```json
{
  "event": "prompt_cached",
  "hash": "a1b2c3d4...",
  "model": "claude-sonnet-4",
  "cost_usd": 0.0234,
  "ttl_hours": 24
}
```

### M√©tricas

```python
from src.core.observability import record_cache_hit, record_cache_miss

# Autom√°tico en PromptCacheService
# Expuesto v√≠a Prometheus metrics
```

### Estad√≠sticas

```python
cache = get_prompt_cache_service()
stats = await cache.get_cache_stats()

print(f"Enabled: {stats['enabled']}")
print(f"TTL: {stats['ttl_hours']} hours")
print(f"Type: {stats['cache_type']}")
```

---

## üîß Configuraci√≥n

### TTL del Cache

El TTL por defecto es **24 horas**. Puedes personalizarlo:

```python
from src.modules.ai.prompt_cache import PROMPT_CACHE_TTL_SECONDS

# Default: 24 horas
print(PROMPT_CACHE_TTL_SECONDS)  # 86400

# Custom TTL al guardar
await cache.set_cached_response(
    ...,
    ttl_seconds=60 * 60 * 12,  # 12 horas
)
```

### Infraestructura

El Prompt Cache usa el `CacheService` existente:

1. **Con Redis**: Cache persistente, compartido entre workers
2. **Sin Redis**: Fallback a memoria (solo worker local)

```python
# Configurar Redis (opcional pero recomendado)
REDIS_URL=redis://localhost:6379/0
```

---

## üí∞ ROI y Ahorro

### Escenario Real: An√°lisis de Coherencia

**Sin cache:**
- Requests/d√≠a: 1,000
- Costo promedio: $0.05 por an√°lisis
- **Total/mes: $1,500 USD**

**Con cache (50% hit rate):**
- Requests cacheadas: 500 √ó $0.00 = $0.00
- Requests nuevas: 500 √ó $0.05 = $25.00
- **Total/mes: $750 USD**
- **Ahorro: $750/mes (50%)**

**Con cache (80% hit rate):**
- Requests cacheadas: 800 √ó $0.00 = $0.00
- Requests nuevas: 200 √ó $0.05 = $10.00
- **Total/mes: $300 USD**
- **Ahorro: $1,200/mes (80%)**

### Speedup

- Cache HIT: ~5ms
- API call: ~1,500ms
- **Speedup: 300x** üöÄ

---

## üéØ Casos de Uso Ideales

### ‚úÖ Usa Prompt Cache Para:

1. **An√°lisis repetidos del mismo proyecto**
   - Usuario revisa proyecto m√∫ltiples veces
   - Mismo prompt, misma respuesta

2. **Clasificaci√≥n de documentos similares**
   - Muchos documentos del mismo tipo
   - Prompts estandarizados

3. **Tareas con temperatura=0.0**
   - Respuestas deterministas
   - Mismo input ‚Üí mismo output

4. **Validaciones frecuentes**
   - Check de formato
   - Validaci√≥n de estructura

5. **APIs p√∫blicas con prompts comunes**
   - M√∫ltiples usuarios, mismas preguntas
   - Cache compartido (con Redis)

### ‚ùå NO uses Prompt Cache Para:

1. **Temperature > 0.0**
   - Respuestas no deterministas
   - Cache no √∫til

2. **Prompts √∫nicos**
   - Cada request diferente
   - 0% hit rate

3. **Datos sensibles one-time**
   - Prompts con PII
   - No beneficio del cache

4. **An√°lisis exploratorios**
   - Usuario experimenta
   - Prompts siempre cambian

---

## üîê Seguridad y Privacidad

### Datos Cacheados

El cache almacena:
- ‚úÖ Hash SHA-256 del input
- ‚úÖ Respuesta de Claude
- ‚úÖ Metadata (tokens, costo, tiempo)

**NO** almacena:
- ‚ùå API keys
- ‚ùå Tenant IDs en la key
- ‚ùå Informaci√≥n de autenticaci√≥n

### Aislamiento por Tenant

El cache es **compartido** entre tenants para maximizar hit rate.

Si necesitas aislamiento:
```python
# Opci√≥n: Incluir tenant_id en el prompt
prompt = f"[Tenant: {tenant_id}] {user_prompt}"
# ‚Üí Hash diferente por tenant
```

### TTL y Expiraci√≥n

- Cache expira en 24h autom√°ticamente
- No hay datos obsoletos >24h
- Limpieza autom√°tica por Redis

### Invalidaci√≥n Manual

```python
cache = get_prompt_cache_service()

# Invalidar entrada espec√≠fica
await cache.invalidate_cache(
    prompt=prompt,
    system_prompt=system_prompt,
    temperature=temperature,
    model=model,
)
```

---

## üìñ Ejemplos Completos

Ver archivo: `apps/api/src/modules/ai/example_prompt_cache.py`

Ejecutar:
```bash
cd apps/api
python -m src.modules.ai.example_prompt_cache
```

Incluye:
- ‚úÖ Cache hit/miss b√°sico
- ‚úÖ Estabilidad del hash SHA-256
- ‚úÖ Control manual del cache
- ‚úÖ Estad√≠sticas y m√©tricas
- ‚úÖ Comparaci√≥n de costos

---

## üîç Troubleshooting

### Problema: Cache nunca hace HIT

**Verificar:**
1. ¬øCache habilitado?
   ```python
   service.prompt_cache.enabled  # True?
   ```

2. ¬øPrompts realmente id√©nticos?
   ```python
   # Verificar hashes
   hash1 = build_prompt_hash(...)
   hash2 = build_prompt_hash(...)
   print(hash1 == hash2)
   ```

3. ¬øTTL expirado?
   - Cache expira en 24h
   - Verificar logs: "prompt_cache_expired"

4. ¬øRedis configurado?
   - Sin Redis, cache solo funciona en mismo worker
   - Configurar `REDIS_URL` en environment

### Problema: Hit rate muy bajo (<10%)

**Causas posibles:**
- Temperature > 0.0 (respuestas no deterministas)
- Prompts siempre √∫nicos
- TTL muy corto
- Cache se reinicia frecuentemente

**Soluci√≥n:**
- Usar temperature=0.0 para tareas repetibles
- Estandarizar prompts cuando sea posible
- Configurar Redis para persistencia

### Problema: Cache usa mucha memoria

**Soluci√≥n:**
- Configurar Redis con pol√≠tica de eviction:
  ```
  maxmemory 1gb
  maxmemory-policy allkeys-lru
  ```
- Reducir TTL si es necesario
- Monitor memoria con `redis-cli INFO memory`

---

## üìö Referencias

- [C√≥digo: prompt_cache.py](./prompt_cache.py)
- [Integraci√≥n: service.py](./service.py)
- [Cache Core: core/cache.py](../../core/cache.py)
- [Ejemplos: example_prompt_cache.py](./example_prompt_cache.py)

---

## üìù Changelog

### v1.0.0 (2026-01-09)

**Implementado:**
- ‚úÖ Hash SHA-256 del input completo
- ‚úÖ TTL de 24 horas
- ‚úÖ Integraci√≥n con AIService
- ‚úÖ Redis + fallback memoria
- ‚úÖ M√©tricas hit/miss autom√°ticas
- ‚úÖ Documentaci√≥n completa
- ‚úÖ Ejemplos de uso

**Estado:** ‚úÖ READY FOR PRODUCTION

**Tarea:** CE-S2-006 - Cach√© de Prompts Id√©nticos

---

**Autor:** C2Pro AI Team
**Versi√≥n:** 1.0.0
**√öltima Actualizaci√≥n:** 2026-01-09
